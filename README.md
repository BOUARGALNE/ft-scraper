# Financial Times News Scraper

---
**Developer Information**
- **Name**: Hamid Bouargalne
- **Email**: bouargalne.hamid@gmail.com
- **Phone**: +212 6 49 94 91 59
---

This README explores the functionalities of this program, as well as the interest of each method developed in the script, and also illustrates the explanation of execution processes and the results obtained.

An automated scraper to extract and save Financial Times articles from their RSS feeds. The system uses a "swarm scraping" approach to process multiple articles in parallel while avoiding detection.

## Features

- **RSS Scraping**: Automatic extraction of article URLs from FT RSS feeds
- **Parallel Scraping**: Concurrent processing of multiple articles to optimize speed
- **Anti-detection**: Use of Puppeteer Stealth and user-agent rotation
- **Paywall Management**: Attempt to bypass paywall overlays
- **Multiple Storage**: Export to JSON and CSV with duplicate detection
- **Scheduling**: Automatic daily execution via cron
- **Error Handling**: Automatic retry and error logging

## ðŸ”§ Dependencies

### Main Dependencies
```json
{
  "puppeteer-extra": "^3.3.6",
  "puppeteer-extra-plugin-stealth": "^2.11.2",
  "rss-parser": "^3.13.0",
  "csv-writer": "^1.6.0",
  "node-cron": "^3.0.3"
}
```

### Installation
```bash
npm install puppeteer-extra puppeteer-extra-plugin-stealth rss-parser mongodb csv-writer node-cron
```

## Architecture and Functions

### 1. Configuration (`config`)
```javascript
const config = {
  feedUrls: ['https://www.ft.com/world?format=rss', 'https://www.ft.com/companies?format=rss'],
  maxConcurrent: 5,
  limit: 20,
  proxies: [null],
  userAgents: [...]
}
```
**Interest**: Centralizes all configurable parameters (RSS feeds, concurrency limit, proxies, user-agents) to facilitate maintenance and adjustments.

### 2. Browser Launch (`launchBrowser`)
```javascript
async function launchBrowser(proxy = null)
```
**Processing Steps**:
- Random selection of a user-agent
- Configuration of Puppeteer arguments (sandbox, proxy)
- Launch in headless mode

**Interest**: Initializes a browser configured to avoid detection with user-agent rotation and proxy support.

### 3. Article Scraping (`scrapeArticle`)
```javascript
async function scrapeArticle(page, url, retries = 2)
```
**Processing Steps**:
1. Navigate to URL with timeout
2. Remove paywall elements via JavaScript
3. Wait for content loading
4. Extract data (title, body, date, author)
5. Automatic retry on failure

**Interest**: Core function that extracts article content while managing obstacles (paywall, timeouts) with a robust retry system.

### 4. URL Retrieval (`getArticleUrls`)
```javascript
async function getArticleUrls(feeds = config.feedUrls, limit = config.limit)
```
**Processing Steps**:
1. Parse each RSS feed
2. Filter today's articles only
3. Limit number of articles per feed
4. Deduplicate URLs

**Interest**: Intelligent collection of URLs to scrape focusing on recent content and avoiding duplicates.

### 5. JSON Storage (`saveToJson`)
```javascript
async function saveToJson(articles)
```
**Processing Steps**:
1. Read existing JSON file
2. Detect already present articles via URL
3. Add only new articles
4. Save with formatting

**Interest**: Maintains a complete history in JSON while avoiding data duplication.

### 6. CSV Storage (`saveToCsv`)
```javascript
async function saveToCsv(articles)
```
**Processing Steps**:
1. Configure CSV writer with headers
2. Read existing CSV to detect duplicates
3. Append new articles only

**Interest**: CSV format for data analysis and import into external tools (Excel, BI tools).

### 7. Swarm Scraping (`swarmScrape`)
```javascript
async function swarmScrape()
```
**Processing Steps**:
1. Launch browser with random proxy
2. Retrieve list of URLs to scrape
3. Process by concurrent batches
4. Create multiple pages for parallelism
5. Simultaneous execution of article scraping
6. Proper closure of pages and browser
7. Save results

**Interest**: Complete process orchestration with performance optimization via parallel processing while respecting server limits.

## ðŸš€ Usage

### Manual Execution
```bash
node scraper.js
```

### With Scheduler
```bash
node scheduler.js
```
The scheduler executes the scraper daily at midnight UTC.

## Data Structure

### JSON Format

The results will be stored in a JSON file, in this format:

```json
{
  "url": "https://www.ft.com/content/...",
  "title": "Article Title",
  "body": "Full article content...",
  "date": "Published date",
  "author": "Author name"
}
```

### CSV Format

As I have added the option to store them in CSV format, in this form:

```csv
URL,Title,Body,Date,Author
https://www.ft.com/content/...,Article Title,Full content...,Date,Author
```

## Advanced Configuration

### Adding Proxies
```javascript
proxies: [
  'http://user:pass@proxy1:8080',
  'http://user:pass@proxy2:8080',
  null // Direct connection
]
```

### Modifying RSS Feeds
```javascript
feedUrls: [
  'https://www.ft.com/world?format=rss',
  'https://www.ft.com/companies?format=rss',
  'https://www.ft.com/markets?format=rss'
]
```

##  Results and Screenshots

After launching the scheduler.js script, we obtain the results perfectly as desired, as illustrated in the following figures:

### Scraper Execution in Action
![Scraper running](screenshot1.png)
*The scraper processes RSS feeds and scrapes articles by batches to optimize performance*

### Extracted Data - JSON Format
![JSON file preview](screenshot2.png)
*Clear data structure with all extracted fields (URL, title, content, date, author)*

