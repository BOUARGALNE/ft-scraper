# Financial Times News Scraper

Un scraper automatis√© pour extraire et sauvegarder les articles du Financial Times √† partir de leurs flux RSS. Le syst√®me utilise une approche de "swarm scraping" pour traiter plusieurs articles en parall√®le tout en √©vitant la d√©tection.

## üìã Fonctionnalit√©s

- **Scraping RSS** : Extraction automatique des URLs d'articles depuis les flux RSS du FT
- **Scraping parall√®le** : Traitement concurrent de plusieurs articles pour optimiser la vitesse
- **Anti-d√©tection** : Utilisation de Puppeteer Stealth et rotation d'user-agents
- **Gestion des paywalls** : Tentative de contournement des overlays de paywall
- **Sauvegarde multiple** : Export en JSON et CSV avec d√©tection des doublons
- **Planification** : Ex√©cution automatique quotidienne via cron
- **Gestion d'erreurs** : Retry automatique et logging des erreurs

## üîß D√©pendances

### Dependencies principales
```json
{
  "puppeteer-extra": "^3.3.6",
  "puppeteer-extra-plugin-stealth": "^2.11.2",
  "rss-parser": "^3.13.0",
  "mongodb": "^6.0.0",
  "csv-writer": "^1.6.0",
  "node-cron": "^3.0.3"
}
```

### Installation
```bash
npm install puppeteer-extra puppeteer-extra-plugin-stealth rss-parser mongodb csv-writer node-cron
```

## üèóÔ∏è Architecture et Fonctions

### 1. Configuration (`config`)
```javascript
const config = {
  feedUrls: ['https://www.ft.com/world?format=rss', 'https://www.ft.com/companies?format=rss'],
  maxConcurrent: 5,
  limit: 20,
  proxies: [null],
  userAgents: [...]
}
```
**Int√©r√™t** : Centralise tous les param√®tres configurables (flux RSS, limite de concurrence, proxies, user-agents) pour faciliter la maintenance et les ajustements.

### 2. Lancement du navigateur (`launchBrowser`)
```javascript
async function launchBrowser(proxy = null)
```
**√âtapes de traitement** :
- S√©lection al√©atoire d'un user-agent
- Configuration des arguments Puppeteer (sandbox, proxy)
- Lancement en mode headless

**Int√©r√™t** : Initialise un navigateur configur√© pour √©viter la d√©tection avec rotation d'user-agents et support proxy.

### 3. Scraping d'article (`scrapeArticle`)
```javascript
async function scrapeArticle(page, url, retries = 2)
```
**√âtapes de traitement** :
1. Navigation vers l'URL avec timeout
2. Suppression des √©l√©ments de paywall via JavaScript
3. Attente pour le chargement du contenu
4. Extraction des donn√©es (titre, corps, date, auteur)
5. Retry automatique en cas d'√©chec

**Int√©r√™t** : Fonction core qui extrait le contenu d'un article en g√©rant les obstacles (paywall, timeouts) avec un syst√®me de retry robuste.

### 4. R√©cup√©ration des URLs (`getArticleUrls`)
```javascript
async function getArticleUrls(feeds = config.feedUrls, limit = config.limit)
```
**√âtapes de traitement** :
1. Parse de chaque flux RSS
2. Filtrage des articles du jour uniquement
3. Limitation du nombre d'articles par flux
4. D√©duplication des URLs

**Int√©r√™t** : Collecte intelligente des URLs √† scraper en se concentrant sur le contenu r√©cent et en √©vitant les doublons.

### 5. Sauvegarde JSON (`saveToJson`)
```javascript
async function saveToJson(articles)
```
**√âtapes de traitement** :
1. Lecture du fichier JSON existant
2. D√©tection des articles d√©j√† pr√©sents via URL
3. Ajout uniquement des nouveaux articles
4. Sauvegarde avec formatage

**Int√©r√™t** : Maintient un historique complet en JSON tout en √©vitant la duplication de donn√©es.

### 6. Sauvegarde CSV (`saveToCsv`)
```javascript
async function saveToCsv(articles)
```
**√âtapes de traitement** :
1. Configuration du writer CSV avec headers
2. Lecture du CSV existant pour d√©tecter les doublons
3. Append des nouveaux articles uniquement

**Int√©r√™t** : Format CSV pour l'analyse de donn√©es et l'import dans des outils externes (Excel, BI tools).

### 7. Scraping en essaim (`swarmScrape`)
```javascript
async function swarmScrape()
```
**√âtapes de traitement** :
1. Lancement du navigateur avec proxy al√©atoire
2. R√©cup√©ration de la liste des URLs √† scraper
3. Traitement par batches concurrents
4. Cr√©ation de pages multiples pour le parall√©lisme
5. Ex√©cution simultan√©e des scraping d'articles
6. Fermeture propre des pages et du navigateur
7. Sauvegarde des r√©sultats

**Int√©r√™t** : Orchestration compl√®te du processus avec optimisation des performances via le traitement parall√®le tout en respectant les limites du serveur.

## üöÄ Utilisation

### Ex√©cution manuelle
```bash
node scraper.js
```

### Avec planificateur (recommand√©)
```bash
node scheduler.js
```
Le planificateur ex√©cute le scraper quotidiennement √† minuit UTC.

## üìÅ Structure des donn√©es

### Format JSON
```json
{
  "url": "https://www.ft.com/content/...",
  "title": "Article Title",
  "body": "Full article content...",
  "date": "Published date",
  "author": "Author name"
}
```

### Format CSV
```csv
URL,Title,Body,Date,Author
https://www.ft.com/content/...,Article Title,Full content...,Date,Author
```

## ‚öôÔ∏è Configuration avanc√©e

### Ajout de proxies
```javascript
proxies: [
  'http://user:pass@proxy1:8080',
  'http://user:pass@proxy2:8080',
  null // Direct connection
]
```

### Modification des flux RSS
```javascript
feedUrls: [
  'https://www.ft.com/world?format=rss',
  'https://www.ft.com/companies?format=rss',
  'https://www.ft.com/markets?format=rss'
]
```

### Ajustement de la concurrence
```javascript
maxConcurrent: 3 // R√©duire pour des sites plus sensibles
```

## üìù Logs et monitoring

- Logs de progression en temps r√©el
- Sauvegarde des erreurs dans `errors.log`
- Compteurs de nouveaux articles sauvegard√©s
- Retry automatique avec logging des tentatives

## ‚ö†Ô∏è Avertissements

- Respectez les conditions d'utilisation du Financial Times
- Utilisez des d√©lais appropri√©s pour √©viter la surcharge des serveurs
- Testez avec des limites faibles avant le d√©ploiement en production
- Le contournement de paywall peut violer les ToS du site

## üîç Troubleshooting

**Probl√®me** : `waitForTimeout is not a function`
**Solution** : Utiliser `new Promise(resolve => setTimeout(resolve, ms))` pour la compatibilit√©

**Probl√®me** : Articles vides ou "No body"
**Solution** : Ajuster les s√©lecteurs CSS dans `scrapeArticle()`

**Probl√®me** : Trop d'erreurs 429 (Rate limiting)
**Solution** : R√©duire `maxConcurrent` et ajouter plus de d√©lais
